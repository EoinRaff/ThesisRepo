{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Heightmap_GAN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNr74m33z+WWgs5wxGp+UAf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EoinRaff/ThesisRepo/blob/dev/Heightmap_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw8hFbVCrsBy",
        "colab_type": "text"
      },
      "source": [
        "# Heightmap GAN\n",
        "\n",
        "This notebook contains a Generative Adversarial Network designed to generate realistic Heightmaps. These heightmaps will be used to produce high quality landscapes in Unreal Engine or Unity (TBD)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6Nixbew7_uL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# helper function for formatting strings\n",
        "\n",
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtYPWykn9OJ7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "3d904ac4-1a32-4464-d337-cec37149ea45"
      },
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    print(\"Note: not using Google CoLab\")\n",
        "    COLAB = False"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "Note: using Google CoLab\n",
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7z8omsHirpLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Reshape, Dropout, Dense, Flatten, BatchNormalization, Activation, ZeroPadding2D\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import UpSampling2D, Conv2D\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import os \n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3oIZddy-zXu",
        "colab_type": "text"
      },
      "source": [
        "Heightmaps courtesy of kaggle: https://www.kaggle.com/tpapp157/earth-terrain-height-and-segmentation-map-images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcFxYS6I84K3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "63533b55-c56b-4513-eafb-913ad08deac9"
      },
      "source": [
        "# Generation resolution - Must be square \n",
        "# Training data is also scaled to this.\n",
        "# Note GENERATE_RES 4 or higher  will blow Google CoLab's memory and have not\n",
        "# been tested extensivly.\n",
        "GENERATE_RES = 3 # Generation resolution factor (1=32, 2=64, 3=96, 4=128, etc.)\n",
        "GENERATE_SQUARE = 32 * GENERATE_RES # rows/cols (should be square)\n",
        "IMAGE_CHANNELS = 1 # only need greyscale for heightmaps\n",
        "\n",
        "# Preview image \n",
        "PREVIEW_ROWS = 4\n",
        "PREVIEW_COLS = 7\n",
        "PREVIEW_MARGIN = 16\n",
        "\n",
        "# Size vector to generate images from\n",
        "SEED_SIZE = 100\n",
        "\n",
        "# Configuration\n",
        "DATA_PATH = '/content/drive/My Drive/Master Thesis/heightmaps'\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 32\n",
        "BUFFER_SIZE = 60000\n",
        "\n",
        "print(f\"Will generate {GENERATE_SQUARE}px square images.\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Will generate 96px square images.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLqAGtrb1p0Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define loss functions for generator and discriminator\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "  real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "  fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "  total_loss = real_loss + fake_loss\n",
        "  return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "  return cross_entropy(tf.ones_like(fake_output), fake_output)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQV1RJ582t44",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Values for Learning Rate need to be tuned based on resolution\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1.5e-04, 0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1.5e-04, 0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7BfCmXF4CKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(images):\n",
        "  seed = tf.random.normal([BATCH_SIZE, SEED_SIZE])\n",
        "\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    generated_images = generator(seed, training=True)\n",
        "\n",
        "    real_output = discriminator(images, training=True)\n",
        "    fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "    gen_loss = generator_loss(fake_output)\n",
        "    disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "    return gen_loss, disc_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mzf1wchi55W4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(dataset, epoch):\n",
        "  fixed_seed = np.random.normal(0, 1, (PREVIEW_ROWS * PREVIEW_COLS, SEED_SIZE))\n",
        "  start = time.time()\n",
        "\n",
        "  for epoch in range epoch:\n",
        "    epoch_start = time.time()\n",
        "\n",
        "    gen_loss_list = []\n",
        "    disc_loss_list = []\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      t = train_step(image_batch)\n",
        "      gen_loss_list.append(t[0])\n",
        "      disc_loss_list.append(t[1])\n",
        "\n",
        "    g_loss = sum(gen_loss_list) / len(gen_loss_list)\n",
        "    d_loss = sum(disc_loss_list) / len(disc_loss_list)\n",
        "\n",
        "    epoch_elapsed = time.time() - epoch_start\n",
        "    print(f'Epoch {epoch+1}, gen_loss={g_loss}, disc_loss={d_loss}, {hms_string(epoch_elapsed)}')\n",
        "\n",
        "    save_images(epoch, fixed_seed)\n",
        "  \n",
        "  elapsed = time.time() - start\n",
        "  print(f'Training time: {hms_string(elapsed)}')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}